{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TOI Scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoMjZy7KyKhGi/7m1BFs5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DesmondWasHere/colab-scraping/blob/main/TOI_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCmv3UJawCOi"
      },
      "source": [
        "# **Step 1: Installing Selnium and Chromium**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUKa1dHoWRQD"
      },
      "source": [
        "## Downloading Selenium and chrome webdriver.\r\n",
        "!pip install selenium\r\n",
        "!apt-get update # to update ubuntu to correctly run apt install\r\n",
        "!apt install chromium-chromedriver\r\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDwbTUHrwTS6"
      },
      "source": [
        "# **Step 2: Importing the header file and setting system path for chromedriver**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rH4KrBhWYgJ"
      },
      "source": [
        "## Header Files\r\n",
        "import sys\r\n",
        "import csv\r\n",
        "import time\r\n",
        "import json\r\n",
        "import pandas as pd\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup \r\n",
        "from selenium import webdriver\r\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlrjdqRdwd2c"
      },
      "source": [
        "# **Step 3: Selnium Settings for colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqiDGR9gWiQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e87eb0-7b99-4042-aae9-f4d7e2fc31c5"
      },
      "source": [
        "## Selnium Custom Settings for colab\r\n",
        "options = webdriver.ChromeOptions()\r\n",
        "options.add_argument('--headless')\r\n",
        "options.add_argument('--no-sandbox')\r\n",
        "options.add_argument('--disable-dev-shm-usage')\r\n",
        "prefs = {\"profile.default_content_setting_values.notifications\" : 2}\r\n",
        "options.add_experimental_option(\"prefs\",prefs)\r\n",
        "driver = webdriver.Chrome(chrome_options=options)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: use options instead of chrome_options\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj4T6mhKDFsT"
      },
      "source": [
        "# **Step 4: Run it for json format.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCHYS_-xWIaU"
      },
      "source": [
        "## Year and month to scrape\r\n",
        "year = input(\"Input the year you want to scrape from, eg. 2015: \")\r\n",
        "month =input(\"Input the month you want to scrape from, eg. 2: \")\r\n",
        "particularDay = int(input(\"Input the day you want to scrape from, eg. 1: \"))\r\n",
        "particularEnd = int(input(\"Input the day you want to scrape till, eg. 10: \"))\r\n",
        "articlePerDay = int(input(\"Enter the number of articles per day (Optimal 10):\"))\r\n",
        "\r\n",
        "## Opening the link\r\n",
        "url = \"https://timesofindia.indiatimes.com/archive/year-{},month-{}.cms\".format(year,month)\r\n",
        "driver.get(url)\r\n",
        "\r\n",
        "## Generating the list of links of day\r\n",
        "dayList = []\r\n",
        "for x in range(2,7):\r\n",
        "    for y in range(1,8):\r\n",
        "        try:\r\n",
        "            day = driver.find_element_by_xpath(\"/html/body/center/div[1]/div[3]/table[2]/tbody/tr[2]/td[1]/div[3]/table/tbody/tr[1]/td/span/div/table/tbody/tr[{}]/td[{}]/a\".format(x,y))\r\n",
        "            dayList.append(day.get_attribute('href'))\r\n",
        "        except:\r\n",
        "            continue\r\n",
        "## Opening a csv file, scraping and storing the values.\r\n",
        "fileName = 'TOIArticleArchive.csv'\r\n",
        "with open(fileName, 'w', newline='') as file:\r\n",
        "    writer = csv.writer(file)\r\n",
        "    writer.writerow([\"Date\",\"Author\",\"Vertical\",\"Headline\",\"Description\"])\r\n",
        "    for d in range(particularDay,particularEnd+1):\r\n",
        "      for day in dayList:\r\n",
        "          if (dayList.index(day) != d-1):\r\n",
        "            print(\"Skipping day\", dayList.index(day)+1)\r\n",
        "            continue\r\n",
        "          ## iterate in dayList\r\n",
        "          print(\"\\n Running on day\",day)\r\n",
        "          articleLinks = []\r\n",
        "          count = 1\r\n",
        "\r\n",
        "          r = requests.get(day)\r\n",
        "\r\n",
        "          soup = BeautifulSoup(r.content, 'html5lib')\r\n",
        "          for a in soup.find_all('tbody')[2].find_all('a',href=True):\r\n",
        "              if a.text:\r\n",
        "                  articleLinks.append(a['href'])\r\n",
        "          print(\"Articles Found\",len(articleLinks))\r\n",
        "          if(len(articleLinks)== 0):\r\n",
        "              print(\"No Articles Here\")\r\n",
        "              continue\r\n",
        "          ## iterate in article Links\r\n",
        "          count = 1\r\n",
        "          for link in range(0,len(articleLinks)):\r\n",
        "            if count>articlePerDay:\r\n",
        "                break\r\n",
        "            try:\r\n",
        "              print(\"Scraping article {} of {}\".format(count, articlePerDay))\r\n",
        "              # Opening an article\r\n",
        "              try:\r\n",
        "                r = requests.get(articleLinks[link])\r\n",
        "              except:\r\n",
        "                print(\"Website error, \",articleLinks[link])\r\n",
        "                continue\r\n",
        "              soup = BeautifulSoup(r.content, 'html5lib')\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              try:\r\n",
        "                day = soup.find('div',attrs = {'class':'as_byline'}).find('div',attrs = {'data-plugin':\"dateformat\"}).text[9:]\r\n",
        "              except:\r\n",
        "                try:\r\n",
        "                  dayStore = soup.find('div',attrs = {'class':'_3Mkg- byline'}).text\r\n",
        "                  day = dayStore[dayStore.rfind(\"|\")+1:]\r\n",
        "                except:\r\n",
        "                  dayStore = soup.find('div',attrs = {'class':'yYIu- byline'}).text\r\n",
        "                  day = dayStore[dayStore.rfind(\"/\")+1:]\r\n",
        "\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              try:\r\n",
        "                author = soup.find('a',attrs = {'class':'auth_detail'}).text ## code for original\r\n",
        "              except:\r\n",
        "                try:\r\n",
        "                  author = soup.find('div',attrs = {'class':'as_byline'}).find('div').text ## code for original\r\n",
        "                except:\r\n",
        "                  try:\r\n",
        "                    author = soup.find('div',attrs = {'class':'_3Mkg- byline'}).find('span').text\r\n",
        "                  except:\r\n",
        "                    try:\r\n",
        "                      dayStore = soup.find('div',attrs = {'class':'_3Mkg- byline'}).text\r\n",
        "                      author = dayStore[:dayStore.find(\"|\")]\r\n",
        "                    except:\r\n",
        "                      dayStore = soup.find('div',attrs = {'class':'yYIu- byline'}).text\r\n",
        "                      author = dayStore[:dayStore.find(\"/\")]\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              vertical = (\" \".join(articleLinks[link].split(\"/\")[5:len(articleLinks[0].split(\"/\"))-3]))\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              headline = soup.find('title').text\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              try:\r\n",
        "                article = soup.find(\"arttextxml\").text\r\n",
        "              except:\r\n",
        "                try:\r\n",
        "                  article = soup.find(\"Normal\").text\r\n",
        "                except:\r\n",
        "                  try:\r\n",
        "                    article = soup.find('div',attrs = {'class':'ga-headlines'}).text\r\n",
        "                  except:\r\n",
        "                    try:\r\n",
        "                      article = soup.find('div',attrs = {'class':'Normal'}).text\r\n",
        "                    except:\r\n",
        "                      article = soup.find('div',attrs = {'class':'_3YYSt clearfix '}).text\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              writer.writerow([day.encode(encoding='UTF-8',errors='ignore'),author.encode(encoding='UTF-8',errors='ignore'),vertical.encode(encoding='UTF-8',errors='ignore'),headline.encode(encoding='UTF-8',errors='ignore'),article.encode(encoding='UTF-8',errors='ignore')])\r\n",
        "              count+=1\r\n",
        "            except:\r\n",
        "              print(\"Skipping \",articleLinks[link],\"Unknown Format\")\r\n",
        "              # pass\r\n",
        "          break\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-ph98jiDgEg"
      },
      "source": [
        "# **Step 5: Run it for xml and json format. (Run Step 4 first.)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp6WYN5jUxr3"
      },
      "source": [
        "csvFile = fileName\r\n",
        "xmlFile = fileName.replace(\".csv\",\".xml\")\r\n",
        "jsonFile = fileName.replace(\".csv\",\".json\")\r\n",
        "\r\n",
        "df = pd.read_csv (fileName)\r\n",
        "df.to_json (jsonFile)\r\n",
        "\r\n",
        "csvData = csv.reader(open(csvFile,encoding='UTF-8',errors='ignore'))\r\n",
        "xmlData = open(xmlFile, 'w')\r\n",
        "xmlData.write('<?xml version=\"1.0\"?>' + \"\\n\")\r\n",
        "# there must be only one top-level tag\r\n",
        "xmlData.write('<csv_data>' + \"\\n\")\r\n",
        "\r\n",
        "rowNum = 0\r\n",
        "for row in csvData:\r\n",
        "    if rowNum == 0:\r\n",
        "        tags = row\r\n",
        "        # replace spaces w/ underscores in tag names\r\n",
        "        for i in range(len(tags)):\r\n",
        "            tags[i] = tags[i].replace(' ', '_')\r\n",
        "    else:\r\n",
        "        xmlData.write('<row>' + \"\\n\")\r\n",
        "        for i in range(len(tags)):\r\n",
        "            xmlData.write('    ' + '<' + tags[i] + '>' \\\r\n",
        "                          + row[i] + '</' + tags[i] + '>' + \"\\n\")\r\n",
        "        xmlData.write('</row>' + \"\\n\")\r\n",
        "\r\n",
        "    rowNum +=1\r\n",
        "\r\n",
        "xmlData.write('</csv_data>' + \"\\n\")\r\n",
        "xmlData.close()\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}