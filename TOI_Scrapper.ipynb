{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TOI Scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcxuLg1o5v92T1ewDhSJgB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DesmondWasHere/colab-scraping/blob/main/TOI_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUKa1dHoWRQD"
      },
      "source": [
        "## Downloading Selenium and chrome webdriver.\r\n",
        "!pip install selenium\r\n",
        "!apt-get update # to update ubuntu to correctly run apt install\r\n",
        "!apt install chromium-chromedriver\r\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rH4KrBhWYgJ"
      },
      "source": [
        "## Header Files\r\n",
        "import sys\r\n",
        "import csv\r\n",
        "import time\r\n",
        "import pandas as pd\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup \r\n",
        "from selenium import webdriver\r\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqiDGR9gWiQk"
      },
      "source": [
        "## Selnium Custom Settings for colab\r\n",
        "options = webdriver.ChromeOptions()\r\n",
        "options.add_argument('--headless')\r\n",
        "options.add_argument('--no-sandbox')\r\n",
        "options.add_argument('--disable-dev-shm-usage')\r\n",
        "prefs = {\"profile.default_content_setting_values.notifications\" : 2}\r\n",
        "options.add_experimental_option(\"prefs\",prefs)\r\n",
        "driver = webdriver.Chrome(chrome_options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCHYS_-xWIaU"
      },
      "source": [
        "## Year and month to scrape\r\n",
        "year = input(\"Input the year you want to scrape from, eg. 2015: \")\r\n",
        "month =input(\"Input the month you want to scrape from, eg. 2: \")\r\n",
        "particularDay = int(input(\"Input the day you want to scrape from, eg. 1: \"))\r\n",
        "particularEnd = int(input(\"Input the day you want to scrape till, eg. 10: \"))\r\n",
        "articlePerDay = int(input(\"Enter the number of articles per day (Optimal 10):\"))\r\n",
        "\r\n",
        "## Opening the link\r\n",
        "url = \"https://timesofindia.indiatimes.com/archive/year-{},month-{}.cms\".format(year,month)\r\n",
        "driver.get(url)\r\n",
        "\r\n",
        "## Generating the list of links of day\r\n",
        "dayList = []\r\n",
        "for x in range(2,7):\r\n",
        "    for y in range(1,8):\r\n",
        "        try:\r\n",
        "            day = driver.find_element_by_xpath(\"/html/body/center/div[1]/div[3]/table[2]/tbody/tr[2]/td[1]/div[3]/table/tbody/tr[1]/td/span/div/table/tbody/tr[{}]/td[{}]/a\".format(x,y))\r\n",
        "            dayList.append(day.get_attribute('href'))\r\n",
        "        except:\r\n",
        "            continue\r\n",
        "## Opening a csv file, scraping and storing the values.\r\n",
        "for d in range(particularDay,particularEnd+1):\r\n",
        "  fileName = 'TOIArticleArchive{}.csv'.format(d)\r\n",
        "  with open(fileName, 'w', newline='') as file:\r\n",
        "      writer = csv.writer(file)\r\n",
        "      writer.writerow([\"Date\",\"Author\",\"Vertical\",\"Headline\",\"Description\"])\r\n",
        "      for day in dayList:\r\n",
        "          if (dayList.index(day) != d-1):\r\n",
        "            print(\"Skipping day\", dayList.index(day)+1)\r\n",
        "            continue\r\n",
        "          ## iterate in dayList\r\n",
        "          print(\"\\n Running on day\",day)\r\n",
        "          articleLinks = []\r\n",
        "          count = 1\r\n",
        "          \r\n",
        "          r = requests.get(day) \r\n",
        "          \r\n",
        "          soup = BeautifulSoup(r.content, 'html5lib') \r\n",
        "          for a in soup.find_all('tbody')[2].find_all('a',href=True):\r\n",
        "              if a.text: \r\n",
        "                  articleLinks.append(a['href'])\r\n",
        "          print(\"Articles Found\",len(articleLinks))\r\n",
        "          if(len(articleLinks)== 0):\r\n",
        "              print(\"No Articles Here\")\r\n",
        "              continue\r\n",
        "          ## iterate in article Links\r\n",
        "          count = 1\r\n",
        "          for link in range(0,len(articleLinks)):\r\n",
        "            if count>articlePerDay:\r\n",
        "                break\r\n",
        "            try:\r\n",
        "              print(\"Scraping article {} of {}\".format(count, articlePerDay))\r\n",
        "              # Opening an article\r\n",
        "              try:\r\n",
        "                r = requests.get(articleLinks[link]) \r\n",
        "              except:\r\n",
        "                print(\"Website error, \",articleLinks[link])\r\n",
        "                continue\r\n",
        "              soup = BeautifulSoup(r.content, 'html5lib') \r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              try:\r\n",
        "                day = soup.find('div',attrs = {'class':'as_byline'}).find('div',attrs = {'data-plugin':\"dateformat\"}).text[9:]\r\n",
        "              except:\r\n",
        "                dayStore = soup.find('div',attrs = {'class':'_3Mkg- byline'}).text\r\n",
        "                day = dayStore[dayStore.rfind(\"|\")+1:]\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              try:\r\n",
        "                author = soup.find('a',attrs = {'class':'auth_detail'}).text ## code for original\r\n",
        "              except:\r\n",
        "                try:\r\n",
        "                  author = soup.find('div',attrs = {'class':'as_byline'}).find('div').text ## code for original\r\n",
        "                except:\r\n",
        "                  try:\r\n",
        "                    author = soup.find('div',attrs = {'class':'_3Mkg- byline'}).find('span').text\r\n",
        "                  except:\r\n",
        "                    dayStore = soup.find('div',attrs = {'class':'_3Mkg- byline'}).text\r\n",
        "                    author = dayStore[:dayStore.find(\"|\")]\r\n",
        "          \r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              vertical = (\" \".join(articleLinks[link].split(\"/\")[5:len(articleLinks[0].split(\"/\"))-3]))\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              headline = soup.find('title').text\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              try:\r\n",
        "                article = soup.find(\"arttextxml\").text\r\n",
        "              except:\r\n",
        "                try:\r\n",
        "                  article = soup.find(\"Normal\").text\r\n",
        "                except:\r\n",
        "                  try:\r\n",
        "                    article = soup.find('div',attrs = {'class':'ga-headlines'}).text\r\n",
        "                  except:\r\n",
        "                    article = soup.find('div',attrs = {'class':'Normal'}).text\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              writer.writerow([day,author,vertical,headline,article])\r\n",
        "              count+=1\r\n",
        "            except:\r\n",
        "              print(\"Skipping \",articleLinks[link],\"Unknown Format\")\r\n",
        "              # pass\r\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}