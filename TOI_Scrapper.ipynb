{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TOI Scrapper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcxuLg1o5v92T1ewDhSJgB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DesmondWasHere/colab-scraping/blob/main/TOI_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUKa1dHoWRQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07b2a49-3df3-480f-c965-c90c2860a8c5"
      },
      "source": [
        "## Downloading Selenium and chrome webdriver.\r\n",
        "!pip install selenium\r\n",
        "!apt-get update # to update ubuntu to correctly run apt install\r\n",
        "!apt install chromium-chromedriver\r\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [41.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,703 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [872 kB]\n",
            "Fetched 2,889 kB in 4s (793 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 17 not upgraded.\n",
            "Need to get 81.0 MB of archives.\n",
            "After this operation, 273 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB]\n",
            "Fetched 81.0 MB in 5s (15.5 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 145480 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rH4KrBhWYgJ"
      },
      "source": [
        "## Header Files\r\n",
        "import sys\r\n",
        "import csv\r\n",
        "import time\r\n",
        "import pandas as pd\r\n",
        "import requests\r\n",
        "from bs4 import BeautifulSoup \r\n",
        "from selenium import webdriver\r\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqiDGR9gWiQk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca366f79-c2a1-47bc-fdfb-888df5c29045"
      },
      "source": [
        "## Selnium Custom Settings for colab\r\n",
        "options = webdriver.ChromeOptions()\r\n",
        "options.add_argument('--headless')\r\n",
        "options.add_argument('--no-sandbox')\r\n",
        "options.add_argument('--disable-dev-shm-usage')\r\n",
        "prefs = {\"profile.default_content_setting_values.notifications\" : 2}\r\n",
        "options.add_experimental_option(\"prefs\",prefs)\r\n",
        "driver = webdriver.Chrome(chrome_options=options)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: use options instead of chrome_options\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCHYS_-xWIaU"
      },
      "source": [
        "## Year and month to scrape\r\n",
        "year = input(\"Input the year you want to scrape from, eg. 2015: \")\r\n",
        "month =input(\"Input the month you want to scrape from, eg. 2: \")\r\n",
        "particularDay = int(input(\"Input the day you want to scrape from, eg. 1: \"))\r\n",
        "particularEnd = int(input(\"Input the day you want to scrape till, eg. 10: \"))\r\n",
        "articlePerDay = int(input(\"Enter the number of articles per day (Optimal 10):\"))\r\n",
        "\r\n",
        "## Opening the link\r\n",
        "url = \"https://timesofindia.indiatimes.com/archive/year-{},month-{}.cms\".format(year,month)\r\n",
        "driver.get(url)\r\n",
        "\r\n",
        "## Generating the list of links of day\r\n",
        "dayList = []\r\n",
        "for x in range(2,7):\r\n",
        "    for y in range(1,8):\r\n",
        "        try:\r\n",
        "            day = driver.find_element_by_xpath(\"/html/body/center/div[1]/div[3]/table[2]/tbody/tr[2]/td[1]/div[3]/table/tbody/tr[1]/td/span/div/table/tbody/tr[{}]/td[{}]/a\".format(x,y))\r\n",
        "            dayList.append(day.get_attribute('href'))\r\n",
        "        except:\r\n",
        "            continue\r\n",
        "## Opening a csv file, scraping and storing the values.\r\n",
        "for d in range(particularDay,particularEnd+1):\r\n",
        "  fileName = 'TOIArticleArchive{}.csv'.format(d)\r\n",
        "  with open(fileName, 'w', newline='') as file:\r\n",
        "      writer = csv.writer(file)\r\n",
        "      writer.writerow([\"Date\",\"Author\",\"Vertical\",\"Headline\",\"Description\"])\r\n",
        "      for day in dayList:\r\n",
        "          if (dayList.index(day) != d-1):\r\n",
        "            print(\"Skipping day\", dayList.index(day)+1)\r\n",
        "            continue\r\n",
        "          ## iterate in dayList\r\n",
        "          print(\"\\n Running on day\",day)\r\n",
        "          articleLinks = []\r\n",
        "          count = 1\r\n",
        "          \r\n",
        "          r = requests.get(day) \r\n",
        "          \r\n",
        "          soup = BeautifulSoup(r.content, 'html5lib') \r\n",
        "          for a in soup.find_all('tbody')[2].find_all('a',href=True):\r\n",
        "              if a.text: \r\n",
        "                  articleLinks.append(a['href'])\r\n",
        "          print(\"Articles Found\",len(articleLinks))\r\n",
        "          if(len(articleLinks)== 0):\r\n",
        "              print(\"No Articles Here\")\r\n",
        "              continue\r\n",
        "          ## iterate in article Links\r\n",
        "          count = 1\r\n",
        "          for link in range(0,len(articleLinks)):\r\n",
        "            if count>articlePerDay:\r\n",
        "                break\r\n",
        "            try:\r\n",
        "              print(\"Scraping article {} of {}\".format(count, articlePerDay))\r\n",
        "              # Opening an article\r\n",
        "              try:\r\n",
        "                r = requests.get(articleLinks[link]) \r\n",
        "              except:\r\n",
        "                print(\"Website error, \",articleLinks[link])\r\n",
        "                continue\r\n",
        "              soup = BeautifulSoup(r.content, 'html5lib') \r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              try:\r\n",
        "                day = soup.find('div',attrs = {'class':'as_byline'}).find('div',attrs = {'data-plugin':\"dateformat\"}).text[9:]\r\n",
        "              except:\r\n",
        "                dayStore = soup.find('div',attrs = {'class':'_3Mkg- byline'}).text\r\n",
        "                day = dayStore[dayStore.rfind(\"|\")+1:]\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              try:\r\n",
        "                author = soup.find('a',attrs = {'class':'auth_detail'}).text ## code for original\r\n",
        "              except:\r\n",
        "                try:\r\n",
        "                  author = soup.find('div',attrs = {'class':'as_byline'}).find('div').text ## code for original\r\n",
        "                except:\r\n",
        "                  try:\r\n",
        "                    author = soup.find('div',attrs = {'class':'_3Mkg- byline'}).find('span').text\r\n",
        "                  except:\r\n",
        "                    dayStore = soup.find('div',attrs = {'class':'_3Mkg- byline'}).text\r\n",
        "                    author = dayStore[:dayStore.find(\"|\")]\r\n",
        "          \r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              vertical = (\" \".join(articleLinks[link].split(\"/\")[5:len(articleLinks[0].split(\"/\"))-3]))\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              headline = soup.find('title').text\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              try:\r\n",
        "                article = soup.find(\"arttextxml\").text\r\n",
        "              except:\r\n",
        "                try:\r\n",
        "                  article = soup.find(\"Normal\").text\r\n",
        "                except:\r\n",
        "                  try:\r\n",
        "                    article = soup.find('div',attrs = {'class':'ga-headlines'}).text\r\n",
        "                  except:\r\n",
        "                    article = soup.find('div',attrs = {'class':'Normal'}).text\r\n",
        "              #~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n",
        "              writer.writerow([day,author,vertical,headline,article])\r\n",
        "              count+=1\r\n",
        "            except:\r\n",
        "              print(\"Skipping \",articleLinks[link],\"Unknown Format\")\r\n",
        "              # pass\r\n",
        "          break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}